{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d79c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Load the pre-trained weights of a DeepLabV3 CNN with a MobileNetV3 backbone\n",
    "weights = None\n",
    "deeplabv3 = None\n",
    "# >>> Set the CNN to evaluation (a.k.a. inference) mode\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Extract the transformation pipeline and the list of categories from the weights object\n",
    "transform = None\n",
    "categories = None\n",
    "# Print the list of catgegories\n",
    "print(f\"categories = {categories}\")\n",
    "# Define a dictionary mapping the category index to an arbitrary color\n",
    "index_to_color = {0: (0, 0, 0),\n",
    "                      1: (255, 0, 0), 2: (0, 255, 0), 3: (0, 0, 255),\n",
    "                      4: (255, 255, 0), 5: (255, 0, 255), 6: (255, 255, 0),\n",
    "                      7: (64, 128, 255), 8: (64, 255, 128), 9: (128, 64, 255),\n",
    "                      10: (128, 255, 64), 11: (255, 64, 128), 12: (255, 128, 64),\n",
    "                      13: (32, 110, 170), 14: (32, 170, 110), 15: (110, 32, 170),\n",
    "                      16: (110, 170, 32), 17: (170, 32, 110), 18: (170, 110, 32),\n",
    "                      19: (100, 140, 200), 20: (100, 200, 140)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6711749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of image URLs\n",
    "image_url_list = [\"https://live.staticflickr.com/6137/6004108221_baed02a7d7_z.jpg\",\n",
    "                 \"https://live.staticflickr.com/5745/21046188832_2c9e0be64c_z.jpg\",\n",
    "                 \"https://live.staticflickr.com/1/1212678_5559f93c91_z.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ebbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the images and pass them through the semantic segmentation CNN\n",
    "original_imgs = []\n",
    "semantic_segmentation_imgs = []\n",
    "with torch.no_grad():\n",
    "    for img_ndx in range(len(image_url_list)):\n",
    "        with urllib.request.urlopen(image_url_list[img_ndx]) as url:\n",
    "            # >>> Load a PIL image from an URL\n",
    "            image_pil = None\n",
    "            original_imgs.append(image_pil)\n",
    "            # >>> Create a batch tensor by passing the PIL image through the transformation pipeline and \n",
    "            # >>> adding a dummy dimension 0 to the image tensor\n",
    "            batch_tsr = None\n",
    "            # >>> Pass the batch tensor through the CNN\n",
    "            output = None\n",
    "            semantic_segmentation_shapeNCHW = output['out'].shape\n",
    "            # Create a semantic segmentation color image\n",
    "            semantic_segmentation_img = np.zeros((semantic_segmentation_shapeNCHW[2], semantic_segmentation_shapeNCHW[3], 3), dtype=np.uint8)\n",
    "            # >>> Create a tensor holding the index of the highest logit. You'll need to use the function torch.argmax()\n",
    "            semantic_segmentation_index_tsr = None\n",
    "            # For each pixel, convert the index to a color\n",
    "            for y in range(semantic_segmentation_img.shape[0]):\n",
    "                for x in range(semantic_segmentation_img.shape[1]):\n",
    "                    index = semantic_segmentation_index_tsr[y, x].item()\n",
    "                    color = index_to_color[index]\n",
    "                    semantic_segmentation_img[y, x, [0, 1, 2]] = color\n",
    "            # >>> Resize semantic_segmentation_img to the original size, without interpolation, i.e. using the nearest neighbor color\n",
    "            width, height = image_pil.size\n",
    "            semantic_segmentation_img = None\n",
    "            semantic_segmentation_imgs.append(semantic_segmentation_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "fig, axs = plt.subplots(nrows=len(original_imgs), ncols=2, figsize=(20, 20))\n",
    "for img_ndx in range(len(original_imgs)):\n",
    "    axs[img_ndx, 0].imshow(original_imgs[img_ndx])\n",
    "    axs[img_ndx, 1].imshow(semantic_segmentation_imgs[img_ndx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e307089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
