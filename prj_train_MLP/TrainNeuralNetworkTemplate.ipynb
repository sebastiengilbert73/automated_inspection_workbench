{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayersPerceptron(torch.nn.Module):\n",
    "    def __init__(self, number_of_inputs=2, hidden_layer_size=3, number_of_outputs=2,\n",
    "                 dropout_ratio=0.5):\n",
    "        super(TwoLayersPerceptron, self).__init__()\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(self.number_of_inputs, self.hidden_layer_size)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_layer_size, self.number_of_outputs)\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_ratio)\n",
    "\n",
    "    def forward(self, input_tsr):  # input_tsr.shape = (N, N_in):\n",
    "        act1 = self.linear1(input_tsr)  # (N, H)\n",
    "        act2 = torch.nn.functional.relu(act1)  # (N, H)\n",
    "        act3 = self.dropout(act2)  # (N, H)\n",
    "        act4 = self.linear2(act3)  # (N, N_out)\n",
    "        return act4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Define a four-layers perceptron\n",
    "class FourLayersPerceptron(torch.nn.Module):\n",
    "    def __init__(self, number_of_inputs=2, hidden_layer1_size=100, hidden_layer2_size=100, \n",
    "                 hidden_layer3_size=100, number_of_outputs=2,\n",
    "                 dropout_ratio=0.5):\n",
    "        super(FourLayersPerceptron, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_tsr):  # input_tsr.shape = (N, N_in):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13894376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to store a list of features and a class index, from a pandas.core.frame.DataFrame\n",
    "class FeaturesAndClass(Dataset):\n",
    "    def __init__(self, dataset_filepath):\n",
    "        super(FeaturesAndClass, self).__init__()\n",
    "        self.dataset_df = pd.read_csv(dataset_filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.dataset_df.iloc[idx]  # Retrieve the observation\n",
    "        features = list(observation[0: -1])  # List of features\n",
    "        input_tsr = torch.tensor(features)  # Tensor of features\n",
    "        class_tensor = torch.tensor(int(observation['class']))  # A tensor containing either 0 or 1\n",
    "        return input_tsr, class_tensor  # Returns the input tensor and the target class index, as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "batch_size = 16\n",
    "dataset_filepath = \"/usercode/images/datasets/sixfeatures_threeclasses.csv\"\n",
    "dataset = FeaturesAndClass(dataset_filepath)\n",
    "# Split the dataset into a training and a validation datasets\n",
    "number_of_validation_observations = round(0.2 * len(dataset))\n",
    "# >>> Split the dataset into train and validation datasets\n",
    "train_dataset, validation_dataset = None, None\n",
    "# >>> Create data loaders\n",
    "train_dataloader = None\n",
    "validation_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753301af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "# >>> Choose one of the two defined architectures (comment the other one)\n",
    "hidden_layer_size = 100\n",
    "dropout_ratio = 0.5\n",
    "neural_net = TwoLayersPerceptron(\n",
    "    number_of_inputs=6,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    number_of_outputs=3,\n",
    "    dropout_ratio=dropout_ratio)\n",
    "\n",
    "\"\"\"hidden_layer1_size = 100\n",
    "hidden_layer2_size = 100\n",
    "hidden_layer3_size = 100\n",
    "dropout_ratio = 0.25\n",
    "neural_net = FourLayersPerceptron(\n",
    "    number_of_inputs=6,\n",
    "    hidden_layer1_size=hidden_layer1_size,\n",
    "    hidden_layer2_size=hidden_layer2_size,\n",
    "    hidden_layer3_size=hidden_layer3_size,\n",
    "    number_of_outputs=3,\n",
    "    dropout_ratio=dropout_ratio)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.003\n",
    "weight_decay = 0.0000001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(neural_net.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6003c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def numberOfCorrectPredictions(predictions_tsr, target_class_tsr):\n",
    "    return sum(torch.argmax(predictions_tsr, dim=1) == target_class_tsr).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd56c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Record statistics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "accuracies = []\n",
    "\n",
    "number_of_epochs = 50\n",
    "for epoch in range(1, number_of_epochs + 1):\n",
    "    # Set the neural network to training mode\n",
    "    neural_net.train()\n",
    "    running_loss = 0.0\n",
    "    number_of_batches = 0\n",
    "    for input_tsr, target_class_tsr in train_dataloader:\n",
    "        # >>> Write the steps that must happen in a batch training loop:\n",
    "        # >>> Set the parameter gradients to zero before every batch\n",
    "        \n",
    "        # >>> # Pass the input tensor through the neural network\n",
    "        \n",
    "        # >>> Compute the loss, i.e., the error function we want to minimize\n",
    "        \n",
    "        # >>> Backpropagate the loss function, to compute the gradient of the loss function with\n",
    "        # respect to every trainable parameter in the neural network\n",
    "        \n",
    "        # >>> Perturb every trainable parameter by a small quantity, in the direction of the steepest loss descent\n",
    "        \n",
    "        # >>> Increment the running loss and the number of batches\n",
    "        \n",
    "        pass\n",
    "    average_training_loss = running_loss/number_of_batches\n",
    "    \n",
    "    # Evaluate with the validation dataset\n",
    "    # Set the neural network to evaluation (inference) mode\n",
    "    neural_net.eval()\n",
    "    validation_running_loss = 0.0\n",
    "    number_of_batches = 0\n",
    "    number_of_correct_predictions = 0\n",
    "    number_of_predictions = 0\n",
    "    for validation_input_tsr, validation_target_output_tsr in validation_dataloader:\n",
    "        # >>> Write the steps that must happen in a batch validation loop:\n",
    "        # >>> Pass the input tensor through the neural network\n",
    "        \n",
    "        # >>> Compute the validation loss\n",
    "        \n",
    "        # >>> Increment validation running loss, the number of correct predictions, \n",
    "        # the number of predictions, and the number of batches\n",
    "        \n",
    "        pass\n",
    "    average_validation_loss = validation_running_loss/number_of_batches\n",
    "    accuracy = number_of_correct_predictions/number_of_predictions\n",
    "    print(f\"Epoch {epoch}: average_training_loss = {average_training_loss}; average_validation_loss = {average_validation_loss}; accuracy = {accuracy}\")\n",
    "    epochs.append(epoch)\n",
    "    train_losses.append(average_training_loss)\n",
    "    validation_losses.append(average_validation_loss)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the metrics evolution\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.plot(epochs, train_losses, color='b', label='Training loss')\n",
    "ax1.plot(epochs, validation_losses, color='r', label='Validation loss')\n",
    "ax1.grid(True)\n",
    "ax1.legend(loc='right')\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('accuracy', color='g')\n",
    "ax2.plot(epochs, accuracies, color='g', label='Accuracy')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970020cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
